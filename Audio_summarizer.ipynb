{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOfQN9wsA6A+Oh9VKUrlUa/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rangasuthan/CISCO-PACKET-TRACER/blob/main/Audio_summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBaKuEBwjaK0",
        "outputId": "4d7635d5-6a5d-4d49-adc9-1118063eb81b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AudioToText'...\n",
            "remote: Enumerating objects: 554, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 554 (delta 26), reused 31 (delta 15), pack-reused 509 (from 1)\u001b[K\n",
            "Receiving objects: 100% (554/554), 19.78 MiB | 7.31 MiB/s, done.\n",
            "Resolving deltas: 100% (318/318), done.\n",
            "Updating files: 100% (107/107), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Carleslc/AudioToText.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title { display-mode: \"form\" }\n",
        "import subprocess\n",
        "\n",
        "from sys import platform as sys_platform\n",
        "\n",
        "status, ffmpeg_version = subprocess.getstatusoutput(\"ffmpeg -version\")\n",
        "\n",
        "if status != 0:\n",
        "  from platform import platform\n",
        "\n",
        "  if sys_platform == 'linux' and 'ubuntu' in platform().lower():\n",
        "    !apt install ffmpeg\n",
        "  else:\n",
        "    print(\"Install ffmpeg: https://ffmpeg.org/download.html\")\n",
        "else:\n",
        "  print(ffmpeg_version.split('\\n')[0])\n",
        "\n",
        "  NO_ROOT_WARNING = '|& grep -v \\\"WARNING: Running pip as the \\'root\\' user\"' # running in Colab\n",
        "\n",
        "  !pip install --no-warn-script-location --user --upgrade pip {NO_ROOT_WARNING}\n",
        "  !pip install --root-user-action=ignore git+https://github.com/openai/whisper.git@v20231117 openai==1.9.0 numpy scipy deepl pydub cohere ffmpeg-python torch==2.1.0 tensorflow-probability==0.23.0 typing-extensions==4.9.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4uDliNtkRcp",
        "outputId": "055ba050-4eef-40e3-88a6-9706353541f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Using cached pip-24.2-py3-none-any.whl.metadata (3.6 kB)\n",
            "Using cached pip-24.2-py3-none-any.whl (1.8 MB)\n",
            "Installing collected packages: pip\n",
            "Successfully installed pip-24.2\n",
            "Collecting git+https://github.com/openai/whisper.git@v20231117\n",
            "  Cloning https://github.com/openai/whisper.git (to revision v20231117) to /tmp/pip-req-build-68k4a768\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-68k4a768\n",
            "  Running command git checkout -q e58f28804528831904c3b6f2c0e473f346223433\n",
            "  Resolved https://github.com/openai/whisper.git to commit e58f28804528831904c3b6f2c0e473f346223433\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting openai==1.9.0\n",
            "  Downloading openai-1.9.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Collecting deepl\n",
            "  Downloading deepl-1.19.1-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting cohere\n",
            "  Downloading cohere-5.11.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting ffmpeg-python\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting torch==2.1.0\n",
            "  Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting tensorflow-probability==0.23.0\n",
            "  Downloading tensorflow_probability-0.23.0-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Collecting typing-extensions==4.9.0\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.9.0) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.9.0) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai==1.9.0)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.9.0) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.9.0) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.9.0) (4.66.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.0)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.0)\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.1.0 (from torch==2.1.0)\n",
            "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (1.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (1.16.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (2.2.1)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (0.6.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability==0.23.0) (0.1.8)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.0) (12.6.68)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.60.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.5.0)\n",
            "Collecting tiktoken (from openai-whisper==20231117)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from deepl) (2.32.3)\n",
            "Collecting boto3<2.0.0,>=1.34.0 (from cohere)\n",
            "  Downloading boto3-1.35.33-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
            "  Downloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting httpx-sse==0.4.0 (from cohere)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting parameterized<0.10.0,>=0.9.0 (from cohere)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.23.4)\n",
            "Collecting sagemaker<3.0.0,>=2.232.1 (from cohere)\n",
            "  Downloading sagemaker-2.232.2-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.19.1)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
            "  Downloading types_requests-2.32.0.20240914-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python) (1.0.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.9.0) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.9.0) (1.2.2)\n",
            "Collecting botocore<1.36.0,>=1.35.33 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading botocore-1.35.33-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.9.0) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai==1.9.0)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.9.0)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.9.0) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->deepl) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->deepl) (2.2.3)\n",
            "Collecting attrs<24,>=23.1.0 (from sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting docker (from sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: google-pasta in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0.0,>=2.232.1->cohere) (0.2.0)\n",
            "Collecting importlib-metadata<7.0,>=1.4.0 (from sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0.0,>=2.232.1->cohere) (4.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0.0,>=2.232.1->cohere) (24.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0.0,>=2.232.1->cohere) (2.2.2)\n",
            "Collecting pathos (from sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading pathos-0.3.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0.0,>=2.232.1->cohere) (4.3.6)\n",
            "Requirement already satisfied: protobuf<5.0,>=3.12 in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0.0,>=2.232.1->cohere) (3.20.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0.0,>=2.232.1->cohere) (5.9.5)\n",
            "Requirement already satisfied: pyyaml~=6.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0.0,>=2.232.1->cohere) (6.0.2)\n",
            "Collecting sagemaker-core<2.0.0,>=1.0.0 (from sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading sagemaker_core-1.0.10-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting sagemaker-mlflow (from sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading sagemaker_mlflow-0.1.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting schema (from sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading schema-0.7.7-py2.py3-none-any.whl.metadata (34 kB)\n",
            "Collecting smdebug-rulesconfig==1.0.1 (from sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl.metadata (943 bytes)\n",
            "Requirement already satisfied: tblib<4,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0.0,>=2.232.1->cohere) (3.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<1,>=0.15->cohere) (0.24.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0) (2.1.5)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.43.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0) (1.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2024.9.11)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.33->boto3<2.0.0,>=1.34.0->cohere) (2.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker<3.0.0,>=2.232.1->cohere) (3.20.2)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere) (13.8.1)\n",
            "Collecting mock<5.0,>4.0 (from sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading mock-4.0.3-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->sagemaker<3.0.0,>=2.232.1->cohere) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->sagemaker<3.0.0,>=2.232.1->cohere) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->sagemaker<3.0.0,>=2.232.1->cohere) (0.20.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->sagemaker<3.0.0,>=2.232.1->cohere) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->sagemaker<3.0.0,>=2.232.1->cohere) (2024.2)\n",
            "Collecting ppft>=1.7.6.9 (from pathos->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading ppft-1.7.6.9-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting dill>=0.3.9 (from pathos->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pox>=0.3.5 (from pathos->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading pox-0.3.5-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting multiprocess>=0.70.17 (from pathos->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting mlflow>=2.8 (from sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading mlflow-2.16.2-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting mlflow-skinny==2.16.2 (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading mlflow_skinny-2.16.2-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (2.2.5)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading alembic-1.13.3-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting graphene<4 (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading graphene-3.3-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (3.7)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (3.7.1)\n",
            "Requirement already satisfied: pyarrow<18,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (16.1.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (1.5.2)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (2.0.35)\n",
            "Collecting gunicorn<24 (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (8.1.7)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading databricks_sdk-0.33.0-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting gitpython<4,>=3.1.9 (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (1.27.0)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (0.5.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere) (2.18.0)\n",
            "Collecting Mako (from alembic!=1.10.0,<2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (3.0.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (2.2.0)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading graphql_core-3.2.4-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting aniso8601<10,>=8 (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (3.1.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (3.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (3.1.1)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.10/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (2.27.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython<4,>=3.1.9->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (1.2.14)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (0.48b0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (1.16.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (0.6.1)\n",
            "Downloading openai-1.9.0-py3-none-any.whl (223 kB)\n",
            "Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_probability-0.23.0-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m143.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m144.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deepl-1.19.1-py3-none-any.whl (35 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading cohere-5.11.0-py3-none-any.whl (249 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Downloading boto3-1.35.33-py3-none-any.whl (139 kB)\n",
            "Downloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "Downloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading sagemaker-2.232.2-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\n",
            "Downloading types_requests-2.32.0.20240914-py3-none-any.whl (15 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "Downloading botocore-1.35.33-py3-none-any.whl (12.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m124.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
            "Downloading sagemaker_core-1.0.10-py3-none-any.whl (388 kB)\n",
            "Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "Downloading pathos-0.3.3-py3-none-any.whl (82 kB)\n",
            "Downloading sagemaker_mlflow-0.1.0-py3-none-any.whl (24 kB)\n",
            "Downloading schema-0.7.7-py2.py3-none-any.whl (18 kB)\n",
            "Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "Downloading mlflow-2.16.2-py3-none-any.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m134.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-2.16.2-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m121.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Downloading multiprocess-0.70.17-py310-none-any.whl (134 kB)\n",
            "Downloading pox-0.3.5-py3-none-any.whl (29 kB)\n",
            "Downloading ppft-1.7.6.9-py3-none-any.whl (56 kB)\n",
            "Downloading alembic-1.13.3-py3-none-any.whl (233 kB)\n",
            "Downloading graphene-3.3-py2.py3-none-any.whl (128 kB)\n",
            "Downloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "Downloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
            "Downloading databricks_sdk-0.33.0-py3-none-any.whl (562 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m563.0/563.0 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "Downloading graphql_core-3.2.4-py3-none-any.whl (203 kB)\n",
            "Downloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=801359 sha256=99197fba5e2e508db4c311a811af80a06fb5215acc35e0f0af1b9a65e99f4aa8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ujm4qu6e/wheels/82/ef/3d/4c2e010696be8ff45a064a7629234956fc2c50b05bb9299d53\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: schema, pydub, aniso8601, typing-extensions, types-requests, triton, tensorflow-probability, smmap, smdebug-rulesconfig, ppft, pox, parameterized, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mock, Mako, jmespath, importlib-metadata, httpx-sse, h11, gunicorn, graphql-core, ffmpeg-python, fastavro, dill, attrs, tiktoken, nvidia-cusolver-cu12, nvidia-cudnn-cu12, multiprocess, httpcore, graphql-relay, gitdb, docker, deepl, botocore, torch, s3transfer, pathos, httpx, graphene, gitpython, databricks-sdk, alembic, openai-whisper, openai, boto3, sagemaker-core, mlflow-skinny, mlflow, sagemaker-mlflow, sagemaker, cohere\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.2\n",
            "    Uninstalling typing_extensions-4.12.2:\n",
            "      Successfully uninstalled typing_extensions-4.12.2\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.24.0\n",
            "    Uninstalling tensorflow-probability-0.24.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.24.0\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.3.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.3.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.3.3\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.6.59\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.6.59:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.6.59\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.68\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.68:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.68\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.68\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.68:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.68\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.1.4\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.1.4:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.1.4\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 8.4.0\n",
            "    Uninstalling importlib_metadata-8.4.0:\n",
            "      Successfully uninstalled importlib_metadata-8.4.0\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 24.2.0\n",
            "    Uninstalling attrs-24.2.0:\n",
            "      Successfully uninstalled attrs-24.2.0\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.4.69\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.4.69:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.4.69\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.4.0.58\n",
            "    Uninstalling nvidia-cudnn-cu12-9.4.0.58:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.4.0.58\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.4.1+cu121\n",
            "    Uninstalling torch-2.4.1+cu121:\n",
            "      Successfully uninstalled torch-2.4.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.4.1+cu121 requires torch==2.4.1, but you have torch 2.1.0 which is incompatible.\n",
            "torchvision 0.19.1+cu121 requires torch==2.4.1, but you have torch 2.1.0 which is incompatible.\n",
            "typeguard 4.3.0 requires typing-extensions>=4.10.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Mako-1.3.5 alembic-1.13.3 aniso8601-9.0.1 attrs-23.2.0 boto3-1.35.33 botocore-1.35.33 cohere-5.11.0 databricks-sdk-0.33.0 deepl-1.19.1 dill-0.3.9 docker-7.1.0 fastavro-1.9.7 ffmpeg-python-0.2.0 gitdb-4.0.11 gitpython-3.1.43 graphene-3.3 graphql-core-3.2.4 graphql-relay-3.2.0 gunicorn-23.0.0 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 httpx-sse-0.4.0 importlib-metadata-6.11.0 jmespath-1.0.1 mlflow-2.16.2 mlflow-skinny-2.16.2 mock-4.0.3 multiprocess-0.70.17 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvtx-cu12-12.1.105 openai-1.9.0 openai-whisper-20231117 parameterized-0.9.0 pathos-0.3.3 pox-0.3.5 ppft-1.7.6.9 pydub-0.25.1 s3transfer-0.10.2 sagemaker-2.232.2 sagemaker-core-1.0.10 sagemaker-mlflow-0.1.0 schema-0.7.7 smdebug-rulesconfig-1.0.1 smmap-5.0.1 tensorflow-probability-0.23.0 tiktoken-0.8.0 torch-2.1.0 triton-2.1.0 types-requests-2.32.0.20240914 typing-extensions-4.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Gag1s2nwmplV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell code is a slightly modified version of DotCSV code in the following Colab along with other references:\n",
        "# https://colab.research.google.com/drive/1CvvYPAFemIZdSOt9fhN541esSlZR7Ic6?usp=sharing\n",
        "\n",
        "try:\n",
        "  import io\n",
        "  import ffmpeg\n",
        "  import numpy as np\n",
        "\n",
        "  # Only available in Google Colab\n",
        "  from google.colab.output import eval_js\n",
        "\n",
        "  from IPython.display import HTML, Audio\n",
        "  from scipy.io.wavfile import write, read as wav_read\n",
        "  from base64 import b64decode\n",
        "  from os.path import isfile\n",
        "\n",
        "  AUDIO_HTML = \"\"\"\n",
        "  <script>\n",
        "  var my_div = document.createElement(\"DIV\");\n",
        "  var my_p = document.createElement(\"P\");\n",
        "  var my_btn = document.createElement(\"BUTTON\");\n",
        "  var t = document.createTextNode(\"Starting recording...\");\n",
        "\n",
        "  my_btn.appendChild(t);\n",
        "  my_div.appendChild(my_btn);\n",
        "  document.body.appendChild(my_div);\n",
        "\n",
        "  var base64data = 0;\n",
        "  var reader;\n",
        "  var recorder, gumStream;\n",
        "  var recordButton = my_btn;\n",
        "\n",
        "  var handleSuccess = function(stream) {\n",
        "    gumStream = stream;\n",
        "    var options = {\n",
        "      bitsPerSecond: 16000,\n",
        "      mimeType : 'audio/webm;codecs=opus' //codecs=pcm\n",
        "    };\n",
        "    recorder = new MediaRecorder(stream, options);\n",
        "    //recorder = new MediaRecorder(stream);\n",
        "\n",
        "    recorder.ondataavailable = function(e) {\n",
        "      var url = URL.createObjectURL(e.data);\n",
        "      var preview = document.createElement('audio');\n",
        "      preview.controls = true;\n",
        "      preview.src = url;\n",
        "      document.body.appendChild(preview);\n",
        "\n",
        "      reader = new FileReader();\n",
        "      reader.readAsDataURL(e.data);\n",
        "      reader.onloadend = function() {\n",
        "        base64data = reader.result;\n",
        "        //console.log(\"reader.onloadend: \" + base64data);\n",
        "      }\n",
        "    };\n",
        "    recorder.start();\n",
        "    recordButton.innerText = \"🔴 Recording... press to STOP\";\n",
        "  };\n",
        "\n",
        "  navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "  function toggleRecording() {\n",
        "    if (recorder && recorder.state == \"recording\") {\n",
        "        recorder.stop();\n",
        "        gumStream.getAudioTracks()[0].stop();\n",
        "        recordButton.innerText = \"Saving the recording... please wait!\";\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // https://stackoverflow.com/a/951057\n",
        "  function sleep(ms) {\n",
        "    return new Promise(resolve => setTimeout(resolve, ms));\n",
        "  }\n",
        "\n",
        "  var data = new Promise(resolve => {\n",
        "    recordButton.onclick = () => {\n",
        "      toggleRecording();\n",
        "\n",
        "      sleep(2000).then(() => {\n",
        "        // wait 2000ms for the data to be available...\n",
        "        //console.log(\"resolve data: \" + base64data);\n",
        "        resolve(base64data.toString());\n",
        "      });\n",
        "    }\n",
        "  });\n",
        "\n",
        "  function doneRecording(recording_file) {\n",
        "    my_div.removeChild(recordButton);\n",
        "    my_p.innerText = recording_file;\n",
        "    my_div.appendChild(my_p);\n",
        "  }\n",
        "\n",
        "  </script>\n",
        "  \"\"\"\n",
        "\n",
        "  def get_audio():\n",
        "    display(HTML(AUDIO_HTML))\n",
        "    data = eval_js(\"data\")\n",
        "    binary = b64decode(data.split(',')[1])\n",
        "\n",
        "    process = (ffmpeg\n",
        "      .input('pipe:0')\n",
        "      .output('pipe:1', format='wav')\n",
        "      .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n",
        "    )\n",
        "    output, err = process.communicate(input=binary)\n",
        "\n",
        "    riff_chunk_size = len(output) - 8\n",
        "    # Break up the chunk size into four bytes, held in b.\n",
        "    q = riff_chunk_size\n",
        "    b = []\n",
        "    for i in range(4):\n",
        "      q, r = divmod(q, 256)\n",
        "      b.append(r)\n",
        "\n",
        "    # Replace bytes 4:8 in proc.stdout with the actual size of the RIFF chunk.\n",
        "    riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "    sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "    return audio, sr\n",
        "\n",
        "  recording_file = \"recording.wav\" #@param {type:\"string\"}\n",
        "\n",
        "  if isfile(recording_file):\n",
        "    print(f\"{recording_file} already exists, if you want to create another recording with the same name, delete it first\")\n",
        "  else:\n",
        "    # record microphone\n",
        "    audio, sr = get_audio()\n",
        "\n",
        "    # write recording\n",
        "    write(recording_file, sr, audio)\n",
        "\n",
        "    eval_js(f'doneRecording(\"{recording_file}\")')\n",
        "except ImportError:\n",
        "  print(\"Recording only available in Google Colab\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxZFEdVpnYV6",
        "outputId": "57e68763-1aaf-470c-d8c9-d0d1242c07ed"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recording only available in Google Colab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGLaGGFpqlQ5",
        "outputId": "9a17c23a-9c83-468f-f101-d8a8da3051e2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-nx7btv23\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-nx7btv23\n",
            "  Resolved https://github.com/openai/whisper.git to commit 25639fc17ddc013d56c594bfbf7644f2185fad84\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (4.66.5)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (10.5.0)\n",
            "Collecting tiktoken (from openai-whisper==20240930)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting triton>=2.0.0 (from openai-whisper==20240930)\n",
            "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper==20240930) (3.16.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (2024.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20240930) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20240930) (1.3.0)\n",
            "Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803321 sha256=99a59791ddc856af9d59e3748ef406c8d26a5d8b3b1e88cebf6e6fa68d460b30\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7n2me9o9/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: triton, tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20240930 tiktoken-0.8.0 triton-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find . -name \"whisper.py\"\n"
      ],
      "metadata": {
        "id": "oKf31cyzp5H-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apem6UY5qJBr",
        "outputId": "64c2fa4e-4c4a-4f3d-c773-774b7d32a62f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.51.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.6.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, subprocess\n",
        "import os\n",
        "import platform as sys_platform\n",
        "import whisper\n",
        "from whisper.utils import format_timestamp, get_writer, WriteTXT\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "  import tensorflow  # required in Colab to avoid protobuf compatibility issues\n",
        "except ImportError:\n",
        "  pass\n",
        "\n",
        "import torch\n",
        "\n",
        "import math\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "# select task\n",
        "\n",
        "task = \"Transcribe\" #@param [\"Transcribe\", \"Translate to English\"]\n",
        "\n",
        "task = \"transcribe\" if task == \"Transcribe\" else \"translate\"\n",
        "\n",
        "# select audio file\n",
        "\n",
        "audio_file = \"obama.mp3\" #@param {type:\"string\"}\n",
        "\n",
        "audio_files = list(map(lambda audio_path: audio_path.strip(), audio_file.split(',')))\n",
        "\n",
        "for audio_path in audio_files:\n",
        "  if not os.path.isfile(audio_path):\n",
        "    raise FileNotFoundError(audio_path)\n",
        "\n",
        "# set model\n",
        "\n",
        "use_model = \"large-v2\" #@param [\"tiny\", \"base\", \"small\", \"medium\", \"large-v1\", \"large-v2\"]\n",
        "\n",
        "# select language\n",
        "\n",
        "language = \"English\" #@param [\"Auto-Detect\", \"Afrikaans\", \"Albanian\", \"Amharic\", \"Arabic\", \"Armenian\", \"Assamese\", \"Azerbaijani\", \"Bashkir\", \"Basque\", \"Belarusian\", \"Bengali\", \"Bosnian\", \"Breton\", \"Bulgarian\", \"Burmese\", \"Castilian\", \"Catalan\", \"Chinese\", \"Croatian\", \"Czech\", \"Danish\", \"Dutch\", \"English\", \"Estonian\", \"Faroese\", \"Finnish\", \"Flemish\", \"French\", \"Galician\", \"Georgian\", \"German\", \"Greek\", \"Gujarati\", \"Haitian\", \"Haitian Creole\", \"Hausa\", \"Hawaiian\", \"Hebrew\", \"Hindi\", \"Hungarian\", \"Icelandic\", \"Indonesian\", \"Italian\", \"Japanese\", \"Javanese\", \"Kannada\", \"Kazakh\", \"Khmer\", \"Korean\", \"Lao\", \"Latin\", \"Latvian\", \"Letzeburgesch\", \"Lingala\", \"Lithuanian\", \"Luxembourgish\", \"Macedonian\", \"Malagasy\", \"Malay\", \"Malayalam\", \"Maltese\", \"Maori\", \"Marathi\", \"Moldavian\", \"Moldovan\", \"Mongolian\", \"Myanmar\", \"Nepali\", \"Norwegian\", \"Nynorsk\", \"Occitan\", \"Panjabi\", \"Pashto\", \"Persian\", \"Polish\", \"Portuguese\", \"Punjabi\", \"Pushto\", \"Romanian\", \"Russian\", \"Sanskrit\", \"Serbian\", \"Shona\", \"Sindhi\", \"Sinhala\", \"Sinhalese\", \"Slovak\", \"Slovenian\", \"Somali\", \"Spanish\", \"Sundanese\", \"Swahili\", \"Swedish\", \"Tagalog\", \"Tajik\", \"Tamil\", \"Tatar\", \"Telugu\", \"Thai\", \"Tibetan\", \"Turkish\", \"Turkmen\", \"Ukrainian\", \"Urdu\", \"Uzbek\", \"Valencian\", \"Vietnamese\", \"Welsh\", \"Yiddish\", \"Yoruba\"]\n",
        "\n",
        "# other parameters\n",
        "\n",
        "prompt = \"\" #@param {type:\"string\"}\n",
        "\n",
        "coherence_preference = \"More coherence, but may repeat text\" #@param [\"More coherence, but may repeat text\", \"Less repetitions, but may have less coherence\"]\n",
        "\n",
        "api_key = '' #@param {type:\"string\"}\n",
        "\n",
        "# detect device\n",
        "\n",
        "if api_key:\n",
        "  print(\"Using API\")\n",
        "\n",
        "  from pydub import AudioSegment\n",
        "  from pydub.silence import split_on_silence\n",
        "else:\n",
        "  DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "  print(f\"Using {'GPU' if DEVICE == 'cuda' else 'CPU ⚠️'}\")\n",
        "\n",
        "  # https://medium.com/analytics-vidhya/the-google-colab-system-specification-check-69d159597417\n",
        "  if DEVICE == \"cuda\":\n",
        "    !nvidia-smi -L\n",
        "  else:\n",
        "    if sys_platform == 'linux':\n",
        "      !lscpu | grep \"Model name\" | awk '{$1=$1};1'\n",
        "\n",
        "    print(\"Not using GPU can result in a very slow execution\")\n",
        "    print(\"Ensure Hardware accelerator by GPU is enabled in Google Colab: Runtime > Change runtime type\")\n",
        "\n",
        "    if use_model not in ['tiny', 'base', 'small']:\n",
        "      print(\"You may also want to try a smaller model (tiny, base, small)\")\n",
        "\n",
        "# display language\n",
        "\n",
        "WHISPER_LANGUAGES = [k.title() for k in whisper.tokenizer.TO_LANGUAGE_CODE.keys()]\n",
        "\n",
        "if language == \"Auto-Detect\":\n",
        "  language = \"detect\"\n",
        "\n",
        "if language and language != \"detect\" and language not in WHISPER_LANGUAGES:\n",
        "  print(f\"\\nLanguage '{language}' is invalid\")\n",
        "  language = \"detect\"\n",
        "\n",
        "if language and language != \"detect\":\n",
        "  print(f\"\\nLanguage: {language}\")\n",
        "\n",
        "# load model\n",
        "\n",
        "if api_key:\n",
        "  print()\n",
        "else:\n",
        "  MODELS_WITH_ENGLISH_VERSION = [\"tiny\", \"base\", \"small\", \"medium\"]\n",
        "\n",
        "  if language == \"English\" and use_model in MODELS_WITH_ENGLISH_VERSION:\n",
        "    use_model += \".en\"\n",
        "\n",
        "  print(f\"\\nLoading {use_model} model... {os.path.expanduser(f'~/.cache/whisper/{use_model}.pt')}\")\n",
        "\n",
        "  model = whisper.load_model(use_model, device=DEVICE)\n",
        "\n",
        "  print(\n",
        "      f\"Model {use_model} is {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
        "      f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,d} parameters.\\n\"\n",
        "  )\n",
        "\n",
        "# set options\n",
        "\n",
        "## https://github.com/openai/whisper/blob/v20231117/whisper/transcribe.py#L37\n",
        "## https://github.com/openai/whisper/blob/v20231117/whisper/decoding.py#L81\n",
        "options = {\n",
        "    'task': task,\n",
        "    'verbose': True,\n",
        "    'fp16': True,\n",
        "    'best_of': 5,\n",
        "    'beam_size': 5,\n",
        "    'patience': None,\n",
        "    'length_penalty': None,\n",
        "    'suppress_tokens': '-1',\n",
        "    'temperature': (0.0, 0.2, 0.4, 0.6, 0.8, 1.0), # float or tuple\n",
        "    'condition_on_previous_text': coherence_preference == \"More coherence, but may repeat text\",\n",
        "    'initial_prompt': prompt or None,\n",
        "    'word_timestamps': False,\n",
        "}\n",
        "\n",
        "if api_key:\n",
        "  api_client = OpenAI(api_key=api_key)\n",
        "\n",
        "  api_supported_formats = ['mp3', 'mp4', 'mpeg', 'mpga', 'm4a', 'wav', 'webm']\n",
        "  api_max_bytes = 25 * 1024 * 1024 # 25 MB\n",
        "\n",
        "  api_transcribe = api_client.audio.transcriptions if task == 'transcribe' else api_client.audio.translations\n",
        "  api_transcribe = api_transcribe.create\n",
        "\n",
        "  api_model = 'whisper-1' # large-v2\n",
        "\n",
        "  # https://platform.openai.com/docs/api-reference/audio?lang=python\n",
        "  api_options = {\n",
        "    'response_format': 'verbose_json',\n",
        "  }\n",
        "\n",
        "  if prompt:\n",
        "    api_options['prompt'] = prompt\n",
        "\n",
        "  api_temperature = options['temperature'][0] if isinstance(options['temperature'], (tuple, list)) else options['temperature']\n",
        "\n",
        "  if isinstance(api_temperature, (float, int)):\n",
        "    api_options['temperature'] = api_temperature\n",
        "  else:\n",
        "    raise ValueError(\"Invalid temperature type, it must be a float or a tuple of floats\")\n",
        "elif DEVICE == 'cpu':\n",
        "  options['fp16'] = False\n",
        "  torch.set_num_threads(os.cpu_count())\n",
        "\n",
        "# execute task\n",
        "# !whisper \"{audio_file}\" --task {task} --model {use_model} --output_dir {output_dir} --device {DEVICE} --verbose {options['verbose']}\n",
        "\n",
        "if task == \"translate\":\n",
        "  print(\"-- TRANSLATE TO ENGLISH --\")\n",
        "else:\n",
        "  print(\"-- TRANSCRIPTION --\")\n",
        "\n",
        "results = {} # audio_path to result\n",
        "\n",
        "for audio_path in audio_files:\n",
        "  print(f\"\\nProcessing: {audio_path}\\n\")\n",
        "\n",
        "  # detect language\n",
        "  detect_language = not language or language == \"detect\"\n",
        "\n",
        "  if not detect_language:\n",
        "    options['language'] = language\n",
        "    source_language_code = whisper.tokenizer.TO_LANGUAGE_CODE.get(language.lower())\n",
        "  elif not api_key:\n",
        "    # load audio and pad/trim it to fit 30 seconds\n",
        "    audio = whisper.load_audio(audio_path)\n",
        "    audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "    # make log-Mel spectrogram and move to the same device as the model\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "    # detect the spoken language\n",
        "    _, probs = model.detect_language(mel)\n",
        "\n",
        "    source_language_code = max(probs, key=probs.get)\n",
        "    options['language'] = whisper.tokenizer.LANGUAGES[source_language_code].title()\n",
        "\n",
        "    print(f\"Detected language: {options['language']}\\n\")\n",
        "\n",
        "  # transcribe\n",
        "  if api_key:\n",
        "    # API\n",
        "    if task == \"transcribe\" and not detect_language:\n",
        "      api_options['language'] = source_language_code\n",
        "\n",
        "    source_audio_name_path, source_audio_ext = os.path.splitext(audio_path)\n",
        "    source_audio_ext = source_audio_ext[1:]\n",
        "\n",
        "    if source_audio_ext in api_supported_formats:\n",
        "      api_audio_path = audio_path\n",
        "      api_audio_ext = source_audio_ext\n",
        "    else:\n",
        "      ## convert audio file to a supported format\n",
        "      if options['verbose']:\n",
        "        print(f\"API supported formats: {','.join(api_supported_formats)}\")\n",
        "        print(f\"Converting {source_audio_ext} audio to a supported format...\")\n",
        "\n",
        "      api_audio_ext = 'mp3'\n",
        "\n",
        "      api_audio_path = f'{source_audio_name_path}.{api_audio_ext}'\n",
        "\n",
        "      subprocess.run(['ffmpeg', '-i', audio_path, api_audio_path], check=True, capture_output=True)\n",
        "\n",
        "      if options['verbose']:\n",
        "        print(api_audio_path, end='\\n\\n')\n",
        "\n",
        "    ## split audio file in chunks\n",
        "    api_audio_chunks = []\n",
        "\n",
        "    audio_bytes = os.path.getsize(api_audio_path)\n",
        "\n",
        "    if audio_bytes >= api_max_bytes:\n",
        "      if options['verbose']:\n",
        "        print(f\"Audio exceeds API maximum allowed file size.\\nSplitting audio in chunks...\")\n",
        "\n",
        "      audio_segment_file = AudioSegment.from_file(api_audio_path, api_audio_ext)\n",
        "\n",
        "      min_chunks = math.ceil(audio_bytes / (api_max_bytes / 2))\n",
        "\n",
        "      # print(f\"Min chunks: {min_chunks}\")\n",
        "\n",
        "      max_chunk_milliseconds = int(len(audio_segment_file) // min_chunks)\n",
        "\n",
        "      # print(f\"Max chunk milliseconds: {max_chunk_milliseconds}\")\n",
        "\n",
        "      def add_chunk(api_audio_chunk):\n",
        "        api_audio_chunk_path = f\"{source_audio_name_path}_{len(api_audio_chunks) + 1}.{api_audio_ext}\"\n",
        "        api_audio_chunk.export(api_audio_chunk_path, format=api_audio_ext)\n",
        "        api_audio_chunks.append(api_audio_chunk_path)\n",
        "\n",
        "      def raw_split(big_chunk):\n",
        "        subchunks = math.ceil(len(big_chunk) / max_chunk_milliseconds)\n",
        "\n",
        "        for subchunk_i in range(subchunks):\n",
        "          chunk_start = max_chunk_milliseconds * subchunk_i\n",
        "          chunk_end = min(max_chunk_milliseconds * (subchunk_i + 1), len(big_chunk))\n",
        "          add_chunk(big_chunk[chunk_start:chunk_end])\n",
        "\n",
        "      non_silent_chunks = split_on_silence(audio_segment_file,\n",
        "                                           seek_step=5, # ms\n",
        "                                           min_silence_len=1250, # ms\n",
        "                                           silence_thresh=-25, # dB\n",
        "                                           keep_silence=True) # needed to aggregate timestamps\n",
        "\n",
        "      # print(f\"Non silent chunks: {len(non_silent_chunks)}\")\n",
        "\n",
        "      current_chunk = non_silent_chunks[0] if non_silent_chunks else audio_segment_file\n",
        "\n",
        "      for next_chunk in non_silent_chunks[1:]:\n",
        "        if len(current_chunk) > max_chunk_milliseconds:\n",
        "          raw_split(current_chunk)\n",
        "          current_chunk = next_chunk\n",
        "        elif len(current_chunk) + len(next_chunk) <= max_chunk_milliseconds:\n",
        "          current_chunk += next_chunk\n",
        "        else:\n",
        "          add_chunk(current_chunk)\n",
        "          current_chunk = next_chunk\n",
        "\n",
        "      if len(current_chunk) > max_chunk_milliseconds:\n",
        "        raw_split(current_chunk)\n",
        "      else:\n",
        "        add_chunk(current_chunk)\n",
        "\n",
        "      if options['verbose']:\n",
        "        print(f'Total chunks: {len(api_audio_chunks)}\\n')\n",
        "    else:\n",
        "      api_audio_chunks.append(api_audio_path)\n",
        "\n",
        "    ## process chunks\n",
        "    result = None\n",
        "\n",
        "    for api_audio_chunk_path in api_audio_chunks:\n",
        "      ## API request\n",
        "      with open(api_audio_chunk_path, 'rb') as api_audio_file:\n",
        "        api_result = api_transcribe(model=api_model, file=api_audio_file, **api_options)\n",
        "        api_result = api_result.model_dump() # to dict\n",
        "\n",
        "      api_segments = api_result['segments']\n",
        "\n",
        "      if result:\n",
        "        ## update timestamps\n",
        "        last_segment_timestamp = result['segments'][-1]['end'] if result['segments'] else 0\n",
        "\n",
        "        for segment in api_segments:\n",
        "          segment['start'] += last_segment_timestamp\n",
        "          segment['end'] += last_segment_timestamp\n",
        "\n",
        "        ## append new segments\n",
        "        result['segments'].extend(api_segments)\n",
        "\n",
        "        if 'duration' in result:\n",
        "          result['duration'] += api_result.get('duration', 0)\n",
        "      else:\n",
        "        ## first request\n",
        "        result = api_result\n",
        "\n",
        "        if detect_language:\n",
        "          print(f\"Detected language: {result['language'].title()}\\n\")\n",
        "\n",
        "      ## display segments\n",
        "      if options['verbose']:\n",
        "        for segment in api_segments:\n",
        "          print(f\"[{format_timestamp(segment['start'])} --> {format_timestamp(segment['end'])}] {segment['text']}\")\n",
        "  else:\n",
        "    # Open-Source\n",
        "    result = whisper.transcribe(model, audio_path, **options)\n",
        "\n",
        "  # fix results formatting\n",
        "  for segment in result['segments']:\n",
        "    segment['text'] = segment['text'].strip()\n",
        "\n",
        "  result['text'] = '\\n'.join(map(lambda segment: segment['text'], result['segments']))\n",
        "\n",
        "  # set results for this audio file\n",
        "  results[audio_path] = result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uN2sqtgsnd8O",
        "outputId": "e176073c-b1a6-4cf2-de51-eb6a4a9fb861"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU\n",
            "GPU 0: Tesla T4 (UUID: GPU-495ce552-bd2f-b1d8-6979-5765fc83b536)\n",
            "\n",
            "Language: English\n",
            "\n",
            "Loading large-v2 model... /root/.cache/whisper/large-v2.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 2.87G/2.87G [01:01<00:00, 50.3MiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model large-v2 is multilingual and has 1,541,384,960 parameters.\n",
            "\n",
            "-- TRANSCRIPTION --\n",
            "\n",
            "Processing: obama.mp3\n",
            "\n",
            "[00:00.000 --> 00:17.000]  Thank you so much. Thank you so much. Thank you. Thank you. Thank you, Dick Durbin. You make us all proud.\n",
            "[00:17.000 --> 00:34.000]  On behalf of the great state of Illinois, crossroads of a nation, land of Lincoln, let me express my deepest gratitude for the privilege of addressing this convention.\n",
            "[00:34.000 --> 00:42.000]  Tonight is a particular honor for me because, let's face it, my presence on this stage is pretty unlikely.\n",
            "[00:42.000 --> 00:53.000]  My father was a foreign student born and raised in a small village in Kenya. He grew up herding goats, went to school in a tin roof shack.\n",
            "[00:53.000 --> 01:03.000]  His father, my grandfather, was a cook, a domestic servant to the British. But my grandfather had larger dreams for his son.\n",
            "[01:03.000 --> 01:18.000]  Through hard work and perseverance, my father got a scholarship to study in a magical place, America, that shone as a beacon of freedom and opportunity to so many who had come before.\n",
            "[01:18.000 --> 01:32.000]  While studying here, my father met my mother. She was born in a town on the other side of the world, in Kansas.\n",
            "[01:32.000 --> 01:45.000]  Her father worked on oil rigs and farms through most of the Depression. The day after Pearl Harbor, my grandfather signed up for duty, joined Patton's Army, marched across Europe.\n",
            "[01:45.000 --> 01:51.000]  Back home, my grandmother raised a baby and went to work on a bomber assembly line.\n",
            "[01:52.000 --> 02:02.000]  After the war, they studied on the GI Bill, bought a house through FHA, and later moved west, all the way to Hawaii, in search of opportunity.\n",
            "[02:02.000 --> 02:10.000]  And they, too, had big dreams for their daughter, a common dream born of two continents.\n",
            "[02:10.000 --> 02:17.000]  My parents shared not only an improbable love, they shared an abiding faith in the possibilities of this nation.\n",
            "[02:17.000 --> 02:26.000]  They would give me an African name, Barack, or Blessed, believing that in a tolerant America, your name is no barrier to success.\n",
            "[02:27.000 --> 02:41.000]  They imagined me going to the best schools in the land, even though they weren't rich, because in a generous America, you don't have to be rich to achieve your potential.\n",
            "[02:41.000 --> 02:52.000]  They're both passed away now. And yet I know that on this night, they looked down on me with great pride.\n",
            "[02:53.000 --> 03:04.000]  They stand here, and I stand here today, grateful for the diversity of my heritage, aware that my parents' dreams live on in my two precious daughters.\n",
            "[03:04.000 --> 03:17.000]  I stand here knowing that my story is part of the larger American story, that I owe a debt to all of those who came before me, and that in no other country on Earth is my story even possible.\n",
            "[03:23.000 --> 03:35.000]  Tonight, we gather to affirm the greatness of our nation. Not because of the height of our skyscrapers, or the power of our military, or the size of our economy.\n",
            "[03:35.000 --> 03:43.000]  Our pride is based on a very simple premise, summed up in a declaration made over 200 years ago.\n",
            "[03:43.000 --> 03:58.000]  We hold these truths to be self-evident, that all men are created equal, that they are endowed by their creator with certain inalienable rights, that among these are life, liberty, and the pursuit of happiness.\n",
            "[03:58.000 --> 04:17.000]  That is the true genius of America, a faith in simple dreams, an insistence on small miracles, that we can tuck in our children at night and know that they are fed and clothed and safe from harm.\n",
            "[04:18.000 --> 04:37.000]  That we can say what we think, write what we think without hearing a sudden knock on the door. That we can have an idea and start our own business without paying a bribe. That we can participate in the political process without fear of retribution, and that our votes will be counted at least most of the time.\n",
            "[04:37.000 --> 04:58.000]  This year, in this election, we are called to reaffirm our values and our commitments, to hold them against a hard reality, and see how we're measuring up to the legacy of our forebearers and the promise of future generations.\n",
            "[04:59.000 --> 05:07.000]  And fellow Americans, Democrats, Republicans, Independents, I say to you tonight, we have more work to do.\n",
            "[05:10.000 --> 05:23.000]  More work to do for the workers I met in Galesburg, Illinois, who are losing their union jobs at the Maytag plant that's moving to Mexico, and now are having to compete with their own children for jobs that pay seven bucks an hour.\n",
            "[05:23.000 --> 05:35.000]  More to do for the father that I met who was losing his job and choking back the tears wondering how he would pay $4,500 a month for the drugs his son needs without the health benefits that he counted on.\n",
            "[05:36.000 --> 05:46.000]  More to do for the young woman in East St. Louis, and thousands more like her, who has the grades, has the drive, has the will, but doesn't have the money to go to college.\n",
            "[05:47.000 --> 05:57.000]  Now, don't get me wrong, the people I meet in small towns and big cities and diners and office parks, they don't expect government to solve all their problems.\n",
            "[05:58.000 --> 06:01.000]  They know they have to work hard to get ahead, and they want to.\n",
            "[06:02.000 --> 06:09.000]  Go into the collar counties around Chicago, and people will tell you they don't want their tax money wasted by a welfare agency or by the Pentagon.\n",
            "[06:10.000 --> 06:21.000]  Go into any inner city neighborhood, and folks will tell you that government alone can't teach our kids to learn.\n",
            "[06:22.000 --> 06:33.000]  They know that parents have to teach that children can't achieve unless we raise their expectations and turn off the television sets and eradicate the slander that says a black youth with a book is acting white.\n",
            "[06:33.000 --> 06:35.000]  They know those things.\n",
            "[06:41.000 --> 06:56.000]  People don't expect government to solve all their problems, but they sense deep in their bones that with just a slight change in priorities, we can make sure that every child in America has a decent shot at life.\n",
            "[06:57.000 --> 06:59.000]  And that the doors of opportunity remain open to all.\n",
            "[06:59.000 --> 07:03.000]  They know we can do better, and they want that choice.\n",
            "[07:04.000 --> 07:07.000]  In this election, we offer that choice.\n",
            "[07:08.000 --> 07:15.000]  Our party has chosen a man to lead us who embodies the best this country has to offer, and that man is John Kerry.\n",
            "[07:16.000 --> 07:29.000]  John Kerry understands the ideals of community, faith, and service because they've defined his life.\n",
            "[07:30.000 --> 07:39.000]  From his heroic service to Vietnam, to his years as a prosecutor and lieutenant governor, through two decades in the United States Senate, he's devoted himself to this country.\n",
            "[07:39.000 --> 07:44.000]  Again and again, we've seen him make tough choices when easier ones were available.\n",
            "[07:45.000 --> 07:49.000]  His values and his record affirm what is best in us.\n",
            "[07:50.000 --> 07:53.000]  John Kerry believes in an America where hard work is rewarded.\n",
            "[07:54.000 --> 08:00.000]  So instead of offering tax breaks to companies shipping jobs overseas, he offers them to companies creating jobs here at home.\n",
            "[08:01.000 --> 08:10.000]  John Kerry believes in an America where all Americans can afford the same health coverage our politicians in Washington have for themselves.\n",
            "[08:11.000 --> 08:21.000]  John Kerry believes in energy independence, so we aren't held hostage to the profits of oil companies or the sabotage of foreign oil fields.\n",
            "[08:22.000 --> 08:35.000]  John Kerry believes in the constitutional freedoms that have made our country the envy of the world, and he will never sacrifice our basic liberties, nor use faith as a wedge to divide us.\n",
            "[08:36.000 --> 08:51.000]  And John Kerry believes that in a dangerous world, war must be an option sometimes, but it should never be the first option.\n",
            "[08:52.000 --> 09:02.000]  You know, a while back I met a young man named Seamus in a VFW hall in East Moline, Illinois.\n",
            "[09:03.000 --> 09:07.000]  He was a good-looking kid, 6'2\", 6'3\", clear-eyed with an easy smile.\n",
            "[09:08.000 --> 09:12.000]  He told me he joined the Marines and was heading to Iraq the following week.\n",
            "[09:13.000 --> 09:22.000]  And as I listened to him explain why he'd enlisted, the absolute faith he had in our country and its leaders, his devotion to duty and service,\n",
            "[09:23.000 --> 09:27.000]  I thought, this young man was all that any of us might ever hope for in a child.\n",
            "[09:28.000 --> 09:33.000]  But then I asked myself, are we serving Seamus as well as he's serving us?\n",
            "[09:34.000 --> 09:44.000]  I thought of the 900 men and women, sons and daughters, husbands and wives, friends and neighbors who won't be returning to their own hometowns.\n",
            "[09:45.000 --> 09:54.000]  I thought of the families I'd met who were struggling to get by without a loved one's full income, or whose loved ones had returned with a limb missing or nerves shattered,\n",
            "[09:54.000 --> 09:59.000]  but still lacked long-term health benefits because they were reservists.\n",
            "[10:00.000 --> 10:10.000]  When we send our young men and women into harm's way, we have a solemn obligation not to fudge the numbers or shade the truth about why they are going,\n",
            "[10:11.000 --> 10:15.000]  to care for their families while they're gone, to tend to the soldiers upon their return,\n",
            "[10:16.000 --> 10:23.000]  and to never, ever go to war without enough troops to win the war, secure the peace, and earn the respect of the world.\n",
            "[10:24.000 --> 10:37.000]  Now, let me be clear, let me be clear.\n",
            "[10:38.000 --> 10:40.000]  We have real enemies in the world.\n",
            "[10:41.000 --> 10:46.000]  These enemies must be found, they must be pursued, and they must be defeated.\n",
            "[10:46.000 --> 10:55.000]  John Kerry knows this, and just as Lieutenant Kerry did not hesitate to risk his life to protect the men who served with him in Vietnam,\n",
            "[10:56.000 --> 11:01.000]  President Kerry will not hesitate one moment to use our military might to keep America safe and secure.\n",
            "[11:02.000 --> 11:11.000]  John Kerry believes in America, and he knows that it's not enough for just some of us to prosper.\n",
            "[11:11.000 --> 11:21.000]  For alongside our famous individualism, there's another ingredient in the American saga, a belief that we're all connected as one people.\n",
            "[11:22.000 --> 11:28.000]  If there's a child on the south side of Chicago who can't read, that matters to me even if it's not my child.\n",
            "[11:29.000 --> 11:38.000]  If there's a senior citizen somewhere who can't pay for their prescription drugs and having to choose between medicine and the rent, that makes my life poorer even if it's not my grandparent.\n",
            "[11:38.000 --> 11:47.000]  If there's an Arab American family being rounded up without benefit of an attorney or due process, that threatens my civil liberties.\n",
            "[11:48.000 --> 12:05.000]  It is that fundamental belief, it is that fundamental belief, I am my brother's keeper, I am my sister's keeper, that makes this country work.\n",
            "[12:06.000 --> 12:14.000]  It's what allows us to pursue our individual dreams and yet still come together as one American family.\n",
            "[12:15.000 --> 12:19.000]  E pluribus unum, out of many, one.\n",
            "[12:20.000 --> 12:25.000]  Now, even as we speak, there are those who are preparing to divide us.\n",
            "[12:26.000 --> 12:31.000]  The spin masters, the negative ad peddlers, who embrace the politics of anything goes.\n",
            "[12:32.000 --> 12:41.000]  Well, I say to them tonight, there is not a liberal America and a conservative America, there is the United States of America.\n",
            "[12:42.000 --> 12:52.000]  There is not a black America and a white America and Latino America and Asian America, there's the United States of America.\n",
            "[12:52.000 --> 13:06.000]  In the end, that's what this election is about.\n",
            "[13:07.000 --> 13:14.000]  Do we participate in a politics of cynicism or do we participate in a politics of hope?\n",
            "[13:14.000 --> 13:18.000]  John Kerry calls on us to hope.\n",
            "[13:19.000 --> 13:21.000]  John Edwards calls on us to hope.\n",
            "[13:22.000 --> 13:24.000]  I'm not talking about blind optimism here.\n",
            "[13:25.000 --> 13:29.000]  The almost willful ignorance that thinks unemployment will go away if we just don't think about it.\n",
            "[13:30.000 --> 13:32.000]  Or healthcare crisis will solve itself if we just ignore it.\n",
            "[13:33.000 --> 13:36.000]  That's not what I'm talking about. I'm talking about something more substantial.\n",
            "[13:37.000 --> 13:40.000]  It's the hope of slaves sitting around a fire singing freedom songs.\n",
            "[13:40.000 --> 13:44.000]  The hope of immigrants setting out for distant shores.\n",
            "[13:45.000 --> 13:50.000]  The hope of a young naval lieutenant bravely patrolling the Mekong Delta.\n",
            "[13:51.000 --> 13:54.000]  The hope of a mill worker's son who dares to defy the odds.\n",
            "[13:55.000 --> 13:59.000]  The hope of a skinny kid with a funny name who believes that America has a place for him too.\n",
            "[13:59.000 --> 14:04.000]  Hope in the face of difficulty. Hope in the face of uncertainty.\n",
            "[14:05.000 --> 14:07.000]  The audacity of hope.\n",
            "[14:08.000 --> 14:10.000]  In the end, that is God's greatest gift to us.\n",
            "[14:11.000 --> 14:12.000]  The bedrock of this nation.\n",
            "[14:13.000 --> 14:15.000]  A belief in things not seen.\n",
            "[14:16.000 --> 14:17.000]  A belief in the future.\n",
            "[14:18.000 --> 14:19.000]  A belief in the future.\n",
            "[14:20.000 --> 14:21.000]  A belief in the future.\n",
            "[14:22.000 --> 14:23.000]  A belief in the future.\n",
            "[14:23.000 --> 14:24.000]  A belief in the future.\n",
            "[14:25.000 --> 14:26.000]  A belief in the future.\n",
            "[14:27.000 --> 14:28.000]  A belief in the future.\n",
            "[14:29.000 --> 14:30.000]  A belief in the future.\n",
            "[14:31.000 --> 14:32.000]  A belief in the future.\n",
            "[14:33.000 --> 14:34.000]  A belief in the future.\n",
            "[14:35.000 --> 14:36.000]  A belief in the future.\n",
            "[14:37.000 --> 14:38.000]  A belief in the future.\n",
            "[14:39.000 --> 14:40.000]  A belief in the future.\n",
            "[14:41.000 --> 14:42.000]  A belief in the future.\n",
            "[14:43.000 --> 14:44.000]  A belief in the future.\n",
            "[14:45.000 --> 14:46.000]  A belief in the future.\n",
            "[14:47.000 --> 14:48.000]  A belief in the future.\n",
            "[14:49.000 --> 14:50.000]  A belief in the future.\n",
            "[14:50.000 --> 14:55.000]  If we stand on the crossroads of history, we can make the right choices and meet the challenges that face us.\n",
            "[14:56.000 --> 15:09.000]  America, tonight, if you feel the same energy that I do, if you feel the same urgency that I do, if you feel the same passion that I do, if you feel the same hopefulness that I do, if we do what we must do,\n",
            "[15:09.000 --> 15:25.000]  then I have no doubt that all across the country, from Florida to Oregon, from Washington to Maine, the people will rise up in November, and John Kerry will be sworn in as president, and John Edwards will be sworn in as vice president,\n",
            "[15:26.000 --> 15:31.000]  and this country will reclaim its promise, and out of this long political darkness, a brighter day will come.\n",
            "[15:32.000 --> 15:34.000]  Thank you very much everybody. God bless you.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KL5nYqXEBryK",
        "outputId": "199f8fdd-4afb-41ed-b213-c7138f4ce5a3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transcription = result['text']"
      ],
      "metadata": {
        "id": "O1yP9fVtQnTv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(transcription)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkh2RFnURCWc",
        "outputId": "c56d7f69-152f-4158-f257-c39153028001"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thank you so much. Thank you so much. Thank you. Thank you. Thank you, Dick Durbin. You make us all proud.\n",
            "On behalf of the great state of Illinois, crossroads of a nation, land of Lincoln, let me express my deepest gratitude for the privilege of addressing this convention.\n",
            "Tonight is a particular honor for me because, let's face it, my presence on this stage is pretty unlikely.\n",
            "My father was a foreign student born and raised in a small village in Kenya. He grew up herding goats, went to school in a tin roof shack.\n",
            "His father, my grandfather, was a cook, a domestic servant to the British. But my grandfather had larger dreams for his son.\n",
            "Through hard work and perseverance, my father got a scholarship to study in a magical place, America, that shone as a beacon of freedom and opportunity to so many who had come before.\n",
            "While studying here, my father met my mother. She was born in a town on the other side of the world, in Kansas.\n",
            "Her father worked on oil rigs and farms through most of the Depression. The day after Pearl Harbor, my grandfather signed up for duty, joined Patton's Army, marched across Europe.\n",
            "Back home, my grandmother raised a baby and went to work on a bomber assembly line.\n",
            "After the war, they studied on the GI Bill, bought a house through FHA, and later moved west, all the way to Hawaii, in search of opportunity.\n",
            "And they, too, had big dreams for their daughter, a common dream born of two continents.\n",
            "My parents shared not only an improbable love, they shared an abiding faith in the possibilities of this nation.\n",
            "They would give me an African name, Barack, or Blessed, believing that in a tolerant America, your name is no barrier to success.\n",
            "They imagined me going to the best schools in the land, even though they weren't rich, because in a generous America, you don't have to be rich to achieve your potential.\n",
            "They're both passed away now. And yet I know that on this night, they looked down on me with great pride.\n",
            "They stand here, and I stand here today, grateful for the diversity of my heritage, aware that my parents' dreams live on in my two precious daughters.\n",
            "I stand here knowing that my story is part of the larger American story, that I owe a debt to all of those who came before me, and that in no other country on Earth is my story even possible.\n",
            "Tonight, we gather to affirm the greatness of our nation. Not because of the height of our skyscrapers, or the power of our military, or the size of our economy.\n",
            "Our pride is based on a very simple premise, summed up in a declaration made over 200 years ago.\n",
            "We hold these truths to be self-evident, that all men are created equal, that they are endowed by their creator with certain inalienable rights, that among these are life, liberty, and the pursuit of happiness.\n",
            "That is the true genius of America, a faith in simple dreams, an insistence on small miracles, that we can tuck in our children at night and know that they are fed and clothed and safe from harm.\n",
            "That we can say what we think, write what we think without hearing a sudden knock on the door. That we can have an idea and start our own business without paying a bribe. That we can participate in the political process without fear of retribution, and that our votes will be counted at least most of the time.\n",
            "This year, in this election, we are called to reaffirm our values and our commitments, to hold them against a hard reality, and see how we're measuring up to the legacy of our forebearers and the promise of future generations.\n",
            "And fellow Americans, Democrats, Republicans, Independents, I say to you tonight, we have more work to do.\n",
            "More work to do for the workers I met in Galesburg, Illinois, who are losing their union jobs at the Maytag plant that's moving to Mexico, and now are having to compete with their own children for jobs that pay seven bucks an hour.\n",
            "More to do for the father that I met who was losing his job and choking back the tears wondering how he would pay $4,500 a month for the drugs his son needs without the health benefits that he counted on.\n",
            "More to do for the young woman in East St. Louis, and thousands more like her, who has the grades, has the drive, has the will, but doesn't have the money to go to college.\n",
            "Now, don't get me wrong, the people I meet in small towns and big cities and diners and office parks, they don't expect government to solve all their problems.\n",
            "They know they have to work hard to get ahead, and they want to.\n",
            "Go into the collar counties around Chicago, and people will tell you they don't want their tax money wasted by a welfare agency or by the Pentagon.\n",
            "Go into any inner city neighborhood, and folks will tell you that government alone can't teach our kids to learn.\n",
            "They know that parents have to teach that children can't achieve unless we raise their expectations and turn off the television sets and eradicate the slander that says a black youth with a book is acting white.\n",
            "They know those things.\n",
            "People don't expect government to solve all their problems, but they sense deep in their bones that with just a slight change in priorities, we can make sure that every child in America has a decent shot at life.\n",
            "And that the doors of opportunity remain open to all.\n",
            "They know we can do better, and they want that choice.\n",
            "In this election, we offer that choice.\n",
            "Our party has chosen a man to lead us who embodies the best this country has to offer, and that man is John Kerry.\n",
            "John Kerry understands the ideals of community, faith, and service because they've defined his life.\n",
            "From his heroic service to Vietnam, to his years as a prosecutor and lieutenant governor, through two decades in the United States Senate, he's devoted himself to this country.\n",
            "Again and again, we've seen him make tough choices when easier ones were available.\n",
            "His values and his record affirm what is best in us.\n",
            "John Kerry believes in an America where hard work is rewarded.\n",
            "So instead of offering tax breaks to companies shipping jobs overseas, he offers them to companies creating jobs here at home.\n",
            "John Kerry believes in an America where all Americans can afford the same health coverage our politicians in Washington have for themselves.\n",
            "John Kerry believes in energy independence, so we aren't held hostage to the profits of oil companies or the sabotage of foreign oil fields.\n",
            "John Kerry believes in the constitutional freedoms that have made our country the envy of the world, and he will never sacrifice our basic liberties, nor use faith as a wedge to divide us.\n",
            "And John Kerry believes that in a dangerous world, war must be an option sometimes, but it should never be the first option.\n",
            "You know, a while back I met a young man named Seamus in a VFW hall in East Moline, Illinois.\n",
            "He was a good-looking kid, 6'2\", 6'3\", clear-eyed with an easy smile.\n",
            "He told me he joined the Marines and was heading to Iraq the following week.\n",
            "And as I listened to him explain why he'd enlisted, the absolute faith he had in our country and its leaders, his devotion to duty and service,\n",
            "I thought, this young man was all that any of us might ever hope for in a child.\n",
            "But then I asked myself, are we serving Seamus as well as he's serving us?\n",
            "I thought of the 900 men and women, sons and daughters, husbands and wives, friends and neighbors who won't be returning to their own hometowns.\n",
            "I thought of the families I'd met who were struggling to get by without a loved one's full income, or whose loved ones had returned with a limb missing or nerves shattered,\n",
            "but still lacked long-term health benefits because they were reservists.\n",
            "When we send our young men and women into harm's way, we have a solemn obligation not to fudge the numbers or shade the truth about why they are going,\n",
            "to care for their families while they're gone, to tend to the soldiers upon their return,\n",
            "and to never, ever go to war without enough troops to win the war, secure the peace, and earn the respect of the world.\n",
            "Now, let me be clear, let me be clear.\n",
            "We have real enemies in the world.\n",
            "These enemies must be found, they must be pursued, and they must be defeated.\n",
            "John Kerry knows this, and just as Lieutenant Kerry did not hesitate to risk his life to protect the men who served with him in Vietnam,\n",
            "President Kerry will not hesitate one moment to use our military might to keep America safe and secure.\n",
            "John Kerry believes in America, and he knows that it's not enough for just some of us to prosper.\n",
            "For alongside our famous individualism, there's another ingredient in the American saga, a belief that we're all connected as one people.\n",
            "If there's a child on the south side of Chicago who can't read, that matters to me even if it's not my child.\n",
            "If there's a senior citizen somewhere who can't pay for their prescription drugs and having to choose between medicine and the rent, that makes my life poorer even if it's not my grandparent.\n",
            "If there's an Arab American family being rounded up without benefit of an attorney or due process, that threatens my civil liberties.\n",
            "It is that fundamental belief, it is that fundamental belief, I am my brother's keeper, I am my sister's keeper, that makes this country work.\n",
            "It's what allows us to pursue our individual dreams and yet still come together as one American family.\n",
            "E pluribus unum, out of many, one.\n",
            "Now, even as we speak, there are those who are preparing to divide us.\n",
            "The spin masters, the negative ad peddlers, who embrace the politics of anything goes.\n",
            "Well, I say to them tonight, there is not a liberal America and a conservative America, there is the United States of America.\n",
            "There is not a black America and a white America and Latino America and Asian America, there's the United States of America.\n",
            "In the end, that's what this election is about.\n",
            "Do we participate in a politics of cynicism or do we participate in a politics of hope?\n",
            "John Kerry calls on us to hope.\n",
            "John Edwards calls on us to hope.\n",
            "I'm not talking about blind optimism here.\n",
            "The almost willful ignorance that thinks unemployment will go away if we just don't think about it.\n",
            "Or healthcare crisis will solve itself if we just ignore it.\n",
            "That's not what I'm talking about. I'm talking about something more substantial.\n",
            "It's the hope of slaves sitting around a fire singing freedom songs.\n",
            "The hope of immigrants setting out for distant shores.\n",
            "The hope of a young naval lieutenant bravely patrolling the Mekong Delta.\n",
            "The hope of a mill worker's son who dares to defy the odds.\n",
            "The hope of a skinny kid with a funny name who believes that America has a place for him too.\n",
            "Hope in the face of difficulty. Hope in the face of uncertainty.\n",
            "The audacity of hope.\n",
            "In the end, that is God's greatest gift to us.\n",
            "The bedrock of this nation.\n",
            "A belief in things not seen.\n",
            "A belief in the future.\n",
            "A belief in the future.\n",
            "A belief in the future.\n",
            "A belief in the future.\n",
            "A belief in the future.\n",
            "A belief in the future.\n",
            "A belief in the future.\n",
            "A belief in the future.\n",
            "A belief in the future.\n",
            "A belief in the future.\n",
            "A belief in the future.\n",
            "A belief in the future.\n",
            "A belief in the future.\n",
            "A belief in the future.\n",
            "A belief in the future.\n",
            "A belief in the future.\n",
            "A belief in the future.\n",
            "A belief in the future.\n",
            "If we stand on the crossroads of history, we can make the right choices and meet the challenges that face us.\n",
            "America, tonight, if you feel the same energy that I do, if you feel the same urgency that I do, if you feel the same passion that I do, if you feel the same hopefulness that I do, if we do what we must do,\n",
            "then I have no doubt that all across the country, from Florida to Oregon, from Washington to Maine, the people will rise up in November, and John Kerry will be sworn in as president, and John Edwards will be sworn in as vice president,\n",
            "and this country will reclaim its promise, and out of this long political darkness, a brighter day will come.\n",
            "Thank you very much everybody. God bless you.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers reportlab\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lm3DHS6fT5ii",
        "outputId": "72547065-b069-47c0-8dc6-90b9b4e6f9ae"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting reportlab\n",
            "  Downloading reportlab-4.2.5-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from reportlab) (10.4.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from reportlab) (5.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Downloading reportlab-4.2.5-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: reportlab\n",
            "Successfully installed reportlab-4.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.pdfgen import canvas\n",
        "\n",
        "# Initialize the summarization pipeline\n",
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "# Use the transcription as the input for summarization\n",
        "ARTICLE = transcription  # Assuming 'transcription' contains the text to summarize\n",
        "\n",
        "# Function to summarize in chunks\n",
        "def summarize_in_chunks(article, chunk_size=1024):\n",
        "    # Split the article into chunks\n",
        "    chunks = [article[i:i + chunk_size] for i in range(0, len(article), chunk_size)]\n",
        "    summaries = []\n",
        "\n",
        "    # Summarize each chunk\n",
        "    for chunk in chunks:\n",
        "        summary = summarizer(chunk, max_length=450, min_length=150, do_sample=False)\n",
        "        summaries.append(summary[0]['summary_text'])\n",
        "\n",
        "    # Combine summaries\n",
        "    combined_summary = \" \".join(summaries)\n",
        "    return combined_summary\n",
        "\n",
        "# Get the final summarized text\n",
        "final_summary = summarize_in_chunks(ARTICLE)\n",
        "\n",
        "# Split the summary into paragraphs\n",
        "paragraphs = final_summary.split('. ')  # Adjust for better paragraph formatting\n",
        "\n",
        "# Create a PDF file\n",
        "pdf_filename = \"summary.pdf\"\n",
        "c = canvas.Canvas(pdf_filename, pagesize=letter)\n",
        "width, height = letter\n",
        "\n",
        "# Set the starting position for the text\n",
        "y_position = height - 40\n",
        "c.setFont(\"Helvetica\", 12)\n",
        "\n",
        "# Add a title\n",
        "c.drawString(72, y_position, \"Summary\")\n",
        "y_position -= 20  # Move down for the next line\n",
        "\n",
        "# Write the paragraphs into the PDF\n",
        "for paragraph in paragraphs:\n",
        "    if y_position < 40:  # Start a new page if there's not enough space\n",
        "        c.showPage()\n",
        "        c.setFont(\"Helvetica\", 12)\n",
        "        y_position = height - 40  # Reset position\n",
        "\n",
        "    c.drawString(72, y_position, paragraph.strip() + \".\")  # Write paragraph\n",
        "    y_position -= 15  # Move down for the next line\n",
        "\n",
        "# Save the PDF file\n",
        "c.save()\n",
        "\n",
        "print(f\"PDF summary saved as '{pdf_filename}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "NZkYQ8XCSElt",
        "outputId": "fd42af04-a418-4a61-c87f-212a32b8bad5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'reportlab'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1c669ccb029f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mreportlab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpagesizes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mletter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mreportlab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdfgen\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcanvas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Initialize the summarization pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'reportlab'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lXylWwlccHT5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}